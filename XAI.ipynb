{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/XAI/blob/main/XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdKWnte2KQrD"
      },
      "source": [
        "# Introduction to Explainable AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIpI-UPGKQrE"
      },
      "source": [
        "Explainable AI (XAI) refers to methods and techniques in the field of artificial intelligence (AI) that make the outputs of machine learning models understandable to humans.\n",
        "\n",
        "As AI systems are deployed in more sensitive and high-stakes areas such as healthcare, finance, and legal systems, the need for accountability, trust, and fairness in AI decisions has become paramount. Here are some reasons why XAI is becoming more important:\n",
        "\n",
        "1. Trust and Reliability\n",
        "2. Compliance and Regulation\n",
        "3. Debugging and Improvement\n",
        "4. Ethical Considerations\n",
        "5. Collaborative Human-AI Decision Making\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlLMyGNtKQrF"
      },
      "source": [
        "# The Balance Between Accuracy and Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzAlW3bYKQrG"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/balance_between_accuracy_and_interprebality.png#:~:text=Copy-,permalink,-%E2%8C%98\" width=\"800\" height=\"500\" alt=\"Balance between accuracy and interpretability of models\">\n",
        "    <figcaption>Balance between accuracy of models and interpretability of models</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ_l9zrwKQrG"
      },
      "source": [
        "The balance between accuracy and interpretability in AI, as illustrated in the image, highlights a fundamental trade-off:\n",
        "\n",
        "**complex models** like neural networks achieve high accuracy but are less interpretable, making it challenging to understand their decision-making processes.\n",
        "\n",
        "**simpler models** like decision trees and linear regression offer greater interpretability, albeit sometimes at the expense of accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Black Box vs. White Box Models"
      ],
      "metadata": {
        "id": "qqY-MP9xVkhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Black Box Models"
      ],
      "metadata": {
        "id": "_OTwnXUWVrY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition:** Models whose internal workings are not transparent or easily interpretable. They are complex, making it difficult to understand how inputs are processed to produce outputs.\n",
        "\n",
        "**Importance:** While they can achieve high accuracy, especially in complex tasks, their lack of transparency is a challenge for trust, accountability, and compliance in critical applications.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "Deep Neural Networks (DNNs): Used for tasks like image recognition and natural language processing, but their multi-layered structure makes them inherently difficult to interpret.\n",
        "\n",
        "Ensemble Models (e.g., Random Forests): Aggregate the decisions of multiple models, which increases predictive power but complicates understanding of the decision-making process."
      ],
      "metadata": {
        "id": "hUbjYEYmVubg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### White Box Models"
      ],
      "metadata": {
        "id": "HQN89ifQV7YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition:** Models that are inherently transparent, with their internal mechanics easily understandable by humans. The process from input to output is clear, making these models preferable for situations requiring interpretability.\n",
        "\n",
        "**Importance:** Essential for applications where understanding the decision-making process is as important as the decision itself, such as in healthcare or criminal justice. They facilitate trust, debugging, and compliance.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "Linear Regression: Provides a clear equation where the contribution of each feature is directly understandable.\n",
        "\n",
        "Decision Trees: Offer a straightforward, rule-based approach to decision-making, where the path from input to output is easily traceable."
      ],
      "metadata": {
        "id": "8d7UH45QVph2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorization in XAI"
      ],
      "metadata": {
        "id": "xUGOw-fWZigC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agnosticity"
      ],
      "metadata": {
        "id": "GrPdzVAvZkZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Model-agnostic`: Applicable to all model types. _These methods can be used with any type of machine learning model without specific tailoring._\n",
        "- `Model-specific`: Only applicable to a specific model type. _These techniques are designed for particular model architectures._\n"
      ],
      "metadata": {
        "id": "DTLQ4ecVZnAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scope"
      ],
      "metadata": {
        "id": "EhnnjluBZoyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Global explanation`: Explaining the whole model. _This provides an overview of how the model makes decisions on a general level._\n",
        "- `Local explanation`: Explaining individual predictions. _Focuses on explaining the decision-making process for a single instance._"
      ],
      "metadata": {
        "id": "SO19mNVUZrCL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2zvzk0UKQrH"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/XAI/main/images/global_vs_local.png#:~:text=Copy-,permalink,-%E2%8C%98\" width=\"400\" height=\"250\" alt=\"The scope of interpretability\">\n",
        "    <figcaption>Global vs Local Interpretability</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data type"
      ],
      "metadata": {
        "id": "PWpoDB49ZtA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Graph`: _Used for models that process data in the form of graphs or networks._\n",
        "- `Image`: _Applicable for models that work with image data._\n",
        "- `Text / Speech`: _Covers models designed to understand and generate text or speech._\n",
        "- `Tabular`: _Refers to models dealing with structured data arranged in tables._\n"
      ],
      "metadata": {
        "id": "gfUPLqbEZwPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation type"
      ],
      "metadata": {
        "id": "c9ftVCt2Zyf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Visual`: _This involves using visual tools to illustrate the model's workings._\n",
        "- `Data points`: _Highlights specific data points to explain model predictions._\n",
        "- `Feature importance`: _Shows which features are most influential in the model's predictions._\n",
        "- `Surrogate models`: _Involves creating a simpler model that approximates the behavior of the complex one._\n"
      ],
      "metadata": {
        "id": "k_zZ1T_gY5Wj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-SMvykEKQrJ"
      },
      "source": [
        "# Tools and Techniques for Explaining AI Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_YDe464KQrJ"
      },
      "source": [
        "## Local Interpretable Model-Agnostic Explanations (LIME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkIyldxuKQrJ"
      },
      "source": [
        "Describe LIME and how it offers insights into the predictions of complex, less interpretable models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Or0uNsVKQrJ"
      },
      "source": [
        "## SHapley Additive exPlanations (SHAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4QyglWBKQrJ"
      },
      "source": [
        "Discuss SHAP and its role in assessing the contribution of features in highly accurate models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ6ckArTKQrJ"
      },
      "source": [
        "## Counterfactual Explanations and Adversarial Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCj7rDf8KQrJ"
      },
      "source": [
        "Explain how counterfactuals and adversarial examples can highlight model vulnerabilities and enhance interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZIVQ5NmKQrK"
      },
      "source": [
        "## Layerwise Relevance Propagation (LRP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWbRD2xiKQrK"
      },
      "source": [
        "LRP is a technique used to explain the predictions of complex models by tracing the prediction back through the layers of the network to the input features, thereby providing a visual map or a set of influential features that led to the decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnBXEpLRKQrK"
      },
      "source": [
        "Explainable AI Demos is an educational or demonstration tool designed to make AI more accessible and understandable to users by visually and interactively showcasing how AI models arrive at their conclusions. There are four different types of demos:\n",
        "\n",
        "- Handwriting Classification: This demo seems to use LRP to explain how a neural network trained on the MNIST dataset predicts handwritten digits. It suggests that users can input their handwriting for the AI to classify and explain.\n",
        "- Image Classification: A more advanced LRP demo for image classification that uses a neural network implemented with Caffe, a deep learning framework. This demo likely illustrates how the AI model determines the content of images.\n",
        "- Text Classification: This is for classifying natural language documents. The neural network provides predictions on the document's semantic category and uses LRP to explain the classification process.\n",
        "- Visual Question Answering: This demo allows users to ask AI questions about an image and receive not only answers but also visual explanations that highlight relevant parts of the image involved in the AI's reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkU2LM1IKQrK"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontents.com/arkeodev/XAI/main/images/text_recognition_and_classification.png#:~:text=Copy-,permalink,-%E2%8C%98\" alt=\"Text Recognition and Classification\" width=\"600\" /></td>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontents.com/arkeodev/XAI/main/images/image_classification.png#:~:text=Copy-,permalink,-%E2%8C%98\" alt=\"Image Classification\" width=\"600\" /></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontents.com/arkeodev/XAI/main/images/mnist_image_recognition.png#:~:text=.-,Copy,-permalink\" alt=\"MNIST Image Recognition\" width=\"600\" /></td>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontents.com/arkeodev/XAI/main/images/question_and_answering.png#:~:text=Copy-,permalink,-%E2%8C%98\" alt=\"Question and Answering\" width=\"600\" /></td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkWqNj7aKQrK"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOn2Sl2eKQrK"
      },
      "source": [
        "- Summarize the importance of XAI and its implications for the development of ethical and transparent AI systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLvReZXMKQrK"
      },
      "source": [
        "# References and Further Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQU7aUAXKQrK"
      },
      "source": [
        "- Interpretable Machine Learning Book: https://christophm.github.io/interpretable-ml-book/\n",
        "- Wonderful video archive of DeepFndr: https://www.youtube.com/playlist?list=PLV8yxwGOxvvovp-j6ztxhF3QcKXT6vORU\n",
        "- “Why Should I Trust You? ” Explaining the Predictions of Any Classifier: https://arxiv.org/pdf/1602.04938.pdf\n",
        "- Explainable AI Demos: https://lrpserver.hhi.fraunhofer.de/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}