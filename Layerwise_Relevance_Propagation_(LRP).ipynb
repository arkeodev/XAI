{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/XAI/blob/main/Layerwise_Relevance_Propagation_(LRP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXID--9mnuDh"
      },
      "source": [
        "# Layerwise Relevance Propagation (LRP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq-9jq0yn2Eq"
      },
      "source": [
        "LRP is a technique used to explain the predictions of complex models by tracing the prediction back through the layers of the network to the input features, thereby providing a visual map or a set of influential features that led to the decision.\n",
        "\n",
        "LRP has a theoretical foundation in Taylor decomposition, which helps explain the contributions of individual components (input features) to a function's (network's) output near a point (the input data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8J6_VqJYH46"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/lrp.png\" width=\"800\" height=\"300\" alt=\"LRP\">\n",
        "    <figcaption>LRP</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical Foundation"
      ],
      "metadata": {
        "id": "TN_KjreG-Iev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Layer-wise Relevance Propagation (LRP) algorithm elucidates the decisions of neural network models by highlighting the input features that contribute to the prediction.\n",
        "\n",
        "At its core, LRP operates by tracing the prediction backward, redistributing the prediction onto the relevant neurons layer by layer in the network.\n",
        "\n",
        "Below, we dissect the mathematics underpinning LRP and its propagation rules, establishing the foundation for its implementation and application."
      ],
      "metadata": {
        "id": "Ku5HDJZIBpkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propagation Rules"
      ],
      "metadata": {
        "id": "E34PjsiDB8-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LRP begins with the prediction output of the network and backpropagates this 'relevance' score to the input features. The mathematical formulation for this process uses the following propagation rule for a neuron *j* in one layer to neurons *k* in the subsequent layer:\n",
        "\n",
        "$$\n",
        "R_j = \\sum_{k} \\left( \\frac{z_{jk}}{\\sum_j z_{jk}} \\right) R_k\n",
        "$$\n",
        "\n",
        "where $( z_{jk} )$ represents the contribution of neuron $( j )$ to the activation of neuron *k*. The relevance scores $( R_k )$ are distributed backwards to neuron *j* ensuring that the total relevance remains conserved throughout the layers."
      ],
      "metadata": {
        "id": "ZomGTGwyB_kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lrp_propagate(relevances_upper_layer, contributions_z_jk):\n",
        "    \"\"\"\n",
        "    Propagates relevance scores from an upper layer to a lower layer using the LRP rule.\n",
        "\n",
        "    Parameters:\n",
        "    - relevances_upper_layer: numpy array of relevance scores for the upper layer neurons.\n",
        "    - contributions_z_jk: numpy 2D array where each element z_jk represents the\n",
        "      contribution of lower layer neuron j to the activation of upper layer neuron k.\n",
        "\n",
        "    Returns:\n",
        "    - relevances_lower_layer: numpy array of computed relevance scores for the lower layer neurons.\n",
        "    \"\"\"\n",
        "    # Compute the sum of contributions for each upper layer neuron\n",
        "    sum_z_jk = np.sum(contributions_z_jk, axis=0, keepdims=True)\n",
        "\n",
        "    # Compute the relevance for each lower layer neuron\n",
        "    relevances_lower_layer = np.sum((contributions_z_jk / sum_z_jk) * relevances_upper_layer, axis=1)\n",
        "\n",
        "    return relevances_lower_layer\n",
        "\n",
        "# Suppose we have 3 neurons in the lower layer and 2 neurons in the upper layer\n",
        "relevances_upper_layer = np.array([1.2, 0.8])  # relevance scores for the upper layer\n",
        "contributions_z_jk = np.array([[0.5, 0.2], [0.3, 0.4], [0.2, 0.4]])  # contributions from lower layer to upper layer\n",
        "\n",
        "# Propagate the relevance scores to the lower layer\n",
        "relevances_lower_layer = lrp_propagate(relevances_upper_layer, contributions_z_jk)\n",
        "print(relevances_lower_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntuhEkyEEbCU",
        "outputId": "5b689e7a-b278-4f5f-b816-00c1c5106f39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.76 0.68 0.56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conservation Property"
      ],
      "metadata": {
        "id": "nwNVn_eMCvJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key aspect of LRP is its conservation property, which ensures that the sum of relevance scores is preserved across layers. Mathematically, this can be expressed as:\n",
        "\n",
        "$$\n",
        "\\sum_j R_j = \\sum_k R_k\n",
        "$$\n",
        "\n",
        "which implies that the total relevance at any layer is equal to the total relevance of the preceding layer, eventually equating to the function output $( f(x) )$."
      ],
      "metadata": {
        "id": "cj6UDBx_Cxzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the above calculation shows the conversation property\n",
        "import numpy as np\n",
        "\n",
        "def check_conservation(relevances_upper_layer, relevances_lower_layer):\n",
        "    \"\"\"\n",
        "    Checks if the sum of relevance scores is preserved from the upper layer to the lower layer.\n",
        "\n",
        "    Parameters:\n",
        "    - relevances_upper_layer: numpy array of relevance scores for the upper layer neurons.\n",
        "    - relevances_lower_layer: numpy array of relevance scores for the lower layer neurons.\n",
        "\n",
        "    Returns:\n",
        "    - Boolean indicating if the conservation property is satisfied.\n",
        "    \"\"\"\n",
        "    return np.isclose(np.sum(relevances_upper_layer), np.sum(relevances_lower_layer))\n",
        "\n",
        "# Check if the conservation property holds\n",
        "conservation_holds = check_conservation(relevances_upper_layer, relevances_lower_layer)\n",
        "print(f\"Conservation Property Holds: {conservation_holds}\")\n",
        "print(f\"Sum of Relevances in Upper Layer: {np.sum(relevances_upper_layer)}\")\n",
        "print(f\"Sum of Relevances in Lower Layer: {np.sum(relevances_lower_layer)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzLQBRB0FE00",
        "outputId": "4fe962d2-76db-4e05-86a1-ff7b67a9ce24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conservation Property Holds: True\n",
            "Sum of Relevances in Upper Layer: 2.0\n",
            "Sum of Relevances in Lower Layer: 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Taylor Decomposition"
      ],
      "metadata": {
        "id": "zVLoaH_9Cz1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LRP can be theoretically justified through the Deep Taylor Decomposition, where the backpropagation of relevance is considered a series of Taylor expansions at each neuron. This perspective allows us to model complex non-linear relationships within the network.\n",
        "\n",
        "The Deep Taylor Decomposition provides a theoretical foundation for LRP by allowing for the approximation of complex, non-linear neural network behaviors with simpler, locally linear functions. This enables a principled way to dissect the network's decision-making process, revealing which input features are most relevant for a given prediction."
      ],
      "metadata": {
        "id": "efqg6PHHG4gT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conceptual Overview"
      ],
      "metadata": {
        "id": "1WLTxCPBG6nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a neural network that makes predictions based on some input features. The prediction could be anything such as whether an image contains a cat. The final output of this network is a complex, non-linear function of the input. LRP aims to explain this prediction by distributing the \"relevance\" of the prediction back through the network to the input features.\n",
        "\n",
        "Deep Taylor Decomposition aids in this process by simplifying how we understand each neuron's contribution to the prediction. Essentially, it breaks down the complex, non-linear relationships into series of locally linear approximations."
      ],
      "metadata": {
        "id": "ul5uzawgHIqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simplified Example"
      ],
      "metadata": {
        "id": "IVwoZk5gHUx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine a very simple neural network with one hidden layer that predicts whether an image contains a cat, based on two features: $(x_1)$ (shape) and $(x_2)$ (color). The final output, $(f(x))$, is the prediction score of the image being a cat.\n",
        "\n",
        "For a specific prediction, say $(f(x) = 0.8)$, indicating a high confidence that the image contains a cat, we want to understand which feature contributed more to this decision."
      ],
      "metadata": {
        "id": "LRZHFo6JHY_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Taylor Expansion at a Neuron"
      ],
      "metadata": {
        "id": "RJ8byUK3HnLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's focus on a neuron in the hidden layer that receives $(x_1)$ and $(x_2)$. Suppose this neuron significantly activates for cat-like shapes but is indifferent to color. At this neuron, we perform a Taylor expansion centered at a reference point (e.g., where $(x_1)$ is a non-cat-like shape, and $(x_2)$ has a neutral color).\n",
        "\n",
        "The first-order Taylor expansion of the neuron's activation might look something like this:\n",
        "\n",
        "$$\n",
        "f(x) \\approx f(a) + (x_1 - a_1)f'(a_1) + (x_2 - a_2)f'(a_2)\n",
        "$$\n",
        "\n",
        "where $(a)$ is the reference point, and $(f')$ represents the partial derivatives of the neuron's activation function with respect to $(x_1)$ and $(x_2)$."
      ],
      "metadata": {
        "id": "cDRC_5g6Hpyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Distributing Relevance"
      ],
      "metadata": {
        "id": "GQbTme2pINJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using DTD, we interpret the terms $((x_1 - a_1)f'(a_1))$ and $((x_2 - a_2)f'(a_2))$ as the contributions of $(x_1)$ and $(x_2)$ to the activation of this neuron. Because our neuron is primarily sensitive to shape $(x_1)$, the contribution from $(x_1)$ will be larger.\n",
        "\n",
        "In the context of LRP, these contributions are considered \"relevance\" scores that are backpropagated to the input features. The relevance score associated with $(x_1)$ would be higher, indicating that shape is more important for this prediction.\n"
      ],
      "metadata": {
        "id": "pU_1Ck1NIQ1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Taylor Decomposition"
      ],
      "metadata": {
        "id": "fLDCGvzOITXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process described simplifies complex functions into locally linear ones, making it easier to distribute relevance scores back to the input features. By performing similar decompositions at each neuron across the network, LRP can trace back the prediction's relevance to the original input features, despite the network's non-linear complexities."
      ],
      "metadata": {
        "id": "EClPE2XcC2bB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LRP Rules for Deep Rectifier Networks"
      ],
      "metadata": {
        "id": "7blWrdIHC6gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rectifier networks, particularly those with ReLU activations, are ubiquitous in modern deep learning applications. For such networks, different LRP rules apply:\n"
      ],
      "metadata": {
        "id": "Qx_f_Td4C9PV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic Rule (LRP-0)"
      ],
      "metadata": {
        "id": "YyqSgbE1C_a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Basic Rule of Layer-wise Relevance Propagation (LRP-0) suggests distributing the relevance scores to input features proportionally to their contribution to neuron activations:\n",
        "\n",
        "$$\n",
        "R_j = \\sum_{k} \\left( \\frac{a_j w_{jk}}{\\sum_{j} a_j w_{jk}} \\right) R_k\n",
        "$$\n",
        "\n",
        " While intuitive and straightforward, applying the Basic Rule uniformly across an entire deep neural network can lead to suboptimal explanations. This is mainly because:\n",
        "\n",
        "1. **Noisy Gradients:** Deep neural networks often exhibit \"noisy gradients,\" a phenomenon where the gradients (derivatives of the output with respect to the input) do not consistently reflect the importance of input features. This noise can be amplified when propagated back through many layers, leading to misleading relevance attributions.\n",
        "\n",
        "2. **Sensitivity to Input Scale:** The Basic Rule effectively multiplies gradients by the input (Gradient × Input). This makes the relevance scores highly sensitive to the scale of the input features, which might not accurately reflect their actual importance in the decision-making process.\n",
        "\n",
        "3. **Overemphasis of Active Features:** By focusing on the contributions as they occur, the Basic Rule can disproportionately emphasize features that are active (non-zero), potentially overlooking the importance of the absence of certain features (or features with a zero value) in the decision-making process.\n",
        "\n",
        "4. **Lack of Sparsity:** The explanations generated using the Basic Rule across the entire network can become overly complex, highlighting too many input features without distinguishing the most critical ones. This can make the explanation less interpretable.\n",
        "\n",
        "Let's implement a simple Python example using the Basic Rule and demonstrate why more sophisticated rules might be necessary for certain layers:"
      ],
      "metadata": {
        "id": "ctmfBpAWDB2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lrp_basic_rule(activations, weights, relevances_upper):\n",
        "    \"\"\"\n",
        "    Applies the Basic LRP-0 Rule to propagate relevances from an upper layer to a lower layer.\n",
        "\n",
        "    Parameters:\n",
        "    - activations: Activation of neurons in the lower layer (numpy array).\n",
        "    - weights: Weights connecting lower layer to upper layer (numpy 2D array).\n",
        "    - relevances_upper: Relevance scores for the upper layer neurons (numpy array).\n",
        "\n",
        "    Returns:\n",
        "    - relevances_lower: Computed relevance scores for the lower layer neurons.\n",
        "    \"\"\"\n",
        "    # Calculate contributions z for each connection\n",
        "    z = activations[:, None] * weights  # Element-wise multiplication for contribution\n",
        "\n",
        "    # Compute the sum of contributions for each upper neuron, adding epsilon for stabilization\n",
        "    sum_z = np.sum(z, axis=0, keepdims=True)\n",
        "\n",
        "    # Compute the proportions of each contribution\n",
        "    proportions = z / sum_z\n",
        "\n",
        "    # Distribute upper layer relevances according to proportions\n",
        "    relevances_lower = np.sum(proportions * relevances_upper, axis=1)\n",
        "\n",
        "    return relevances_lower\n",
        "\n",
        "activations = np.array([0.5, 0.2, 0.3])\n",
        "weights = np.array([[1, -1], [-1, 1], [0.5, 0.5]])\n",
        "relevances_upper = np.array([0.6, 0.4])\n",
        "\n",
        "relevances_lower = lrp_basic_rule(activations, weights, relevances_upper)\n",
        "print(\"Relevances Lower Layer:\", relevances_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaGe3C6FK-7_",
        "outputId": "afe4c850-4e64-42d1-d6b7-b9f971804b80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevances Lower Layer: [ 2.  -0.8 -0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Basic LRP rule, you see that the first neuron in the lower layer receives a positive relevance score, indicating a significant contribution towards the prediction made by the upper layer. In contrast, the second and third neurons receive negative relevance scores, suggesting their contributions were counter to the prediction or they contributed to decreasing the activation for the class predicted by the upper layer. The basic rule closely follows the direct contributions without stabilization, making the relevance attribution sensitive to the direction (positive or negative) of each neuron's contribution."
      ],
      "metadata": {
        "id": "GsKUHVcaSxzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why More Robust Rules are Necessary**\n",
        "\n",
        "In the example above, the Basic Rule redistributes relevances simply based on the contributions (activations times weights) of lower-layer neurons to upper-layer neurons. While this seems logical, it doesn't account for the potential issues discussed earlier. For deep networks or networks with specific architectural nuances (like ReLU activations introducing non-linearities), applying more nuanced rules that consider these factors can result in more accurate and interpretable relevance attributions.\n",
        "\n",
        "For instance, adding stabilization (LRP-ε) can mitigate the impact of noisy gradients, and focusing on positive contributions (LRP-γ) can help highlight features that actively drive the prediction, offering a more nuanced and interpretable explanation."
      ],
      "metadata": {
        "id": "Kote46PtLGyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epsilon Rule (LRP-ε)"
      ],
      "metadata": {
        "id": "ZNO90D3xDEO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The epsilon rule introduces a stabilization term $( \\epsilon )$ to avoid division by numbers close to zero, which adds robustness:\n",
        "\n",
        "$$\n",
        "R_j = \\sum_{k} \\left( \\frac{a_j w_{jk}}{\\epsilon + \\sum_{j} a_j w_{jk}} \\right) R_k\n",
        "$$"
      ],
      "metadata": {
        "id": "MewSORItDGi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The role of ε is to absorb some relevance when the contributions to the activation of neuron k are weak or contradictory. As ε becomes larger, only the most salient explanation factors survive the absorption. This typically leads to explanations that are sparser in terms of input features and less noisy."
      ],
      "metadata": {
        "id": "Af4EEVEqQJ-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lrp_epsilon_rule(activations, weights, relevances_upper, epsilon=1e-9):\n",
        "    \"\"\"\n",
        "    Applies the LRP-epsilon Rule to propagate relevances from an upper layer to a lower layer.\n",
        "\n",
        "    Parameters:\n",
        "    - activations: Activation of neurons in the lower layer (numpy array).\n",
        "    - weights: Weights connecting lower layer to upper layer (numpy 2D array).\n",
        "    - relevances_upper: Relevance scores for the upper layer neurons (numpy array).\n",
        "    - epsilon: Stabilization term to avoid division by close-to-zero numbers.\n",
        "\n",
        "    Returns:\n",
        "    - relevances_lower: Computed relevance scores for the lower layer neurons.\n",
        "    \"\"\"\n",
        "    # Calculate contributions z for each connection\n",
        "    z = activations[:, None] * weights  # Element-wise multiplication for contribution\n",
        "\n",
        "    # Compute the sum of contributions for each upper neuron, adding epsilon for stabilization\n",
        "    sum_z = np.sum(z, axis=0, keepdims=True) + epsilon\n",
        "\n",
        "    # Compute the proportions of each contribution\n",
        "    proportions = z / sum_z\n",
        "\n",
        "    # Distribute upper layer relevances according to proportions\n",
        "    relevances_lower = np.sum(proportions * relevances_upper, axis=1)\n",
        "\n",
        "    return relevances_lower\n",
        "\n",
        "activations = np.array([0.5, 0.2, 0.3])  # Example activations from the lower layer\n",
        "weights = np.array([[1, -1], [-1, 1], [0.5, 0.5]])  # Example weights to upper layer\n",
        "relevances_upper = np.array([0.6, 0.4])  # Example relevance scores from the upper layer\n",
        "\n",
        "# Apply the LRP-epsilon rule\n",
        "relevances_lower = lrp_epsilon_rule(activations, weights, relevances_upper, epsilon=0.1)\n",
        "print(\"Relevances Lower Layer:\", relevances_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_zEV5gkRCyB",
        "outputId": "9faa3945-c4cf-4c49-d14c-778b1d2db457"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevances Lower Layer: [ 4.54545455 -1.81818182 -1.03636364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the Epsilon rule, all relevance scores also reflect the direction of contribution but are adjusted by the stabilization term, epsilon. This term prevents extreme distribution of relevance scores caused by small denominators. The relevance scores are somewhat \"smoothed,\" and while the overall direction of contribution (positive or negative) remains the same for each neuron as with the Basic rule, the magnitude of each score is adjusted. The first neuron still shows a strong positive relevance, indicating a significant contribution to the prediction, while the second and third neurons' negative relevances are moderated but still indicate a detracting contribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "svyF3zvES51M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gamma Rule (LRP-γ)"
      ],
      "metadata": {
        "id": "3qwbiBTvDJVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gamma rule emphasizes positive contributions by increasing the weights associated with positive activations:\n",
        "\n",
        "$$\n",
        "R_j = \\sum_{k} \\left( \\frac{a_j (w_{jk} + \\gamma w_{jk}^+)}{\\sum_{j} a_j (w_{jk} + \\gamma w_{jk}^+)} \\right) R_k\n",
        "$$\n",
        "\n",
        "The parameter $(\\gamma)$ controls by how much positive contributions are favored. As $(\\gamma)$ increases, negative contributions start to disappear."
      ],
      "metadata": {
        "id": "snPZO35gDLza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def lrp_gamma_rule(activations, weights, relevances_upper, gamma=0.1):\n",
        "    \"\"\"\n",
        "    Applies the LRP-gamma Rule to propagate relevances from an upper layer to a lower layer.\n",
        "\n",
        "    Parameters:\n",
        "    - activations: Activation of neurons in the lower layer (numpy array).\n",
        "    - weights: Weights connecting lower layer to upper layer (numpy 2D array).\n",
        "    - relevances_upper: Relevance scores for the upper layer neurons (numpy array).\n",
        "    - gamma: Factor to increase the weights of positive activations.\n",
        "\n",
        "    Returns:\n",
        "    - relevances_lower: Computed relevance scores for the lower layer neurons.\n",
        "    \"\"\"\n",
        "    # Identifying positive weights\n",
        "    weights_positive = np.maximum(0, weights)\n",
        "\n",
        "    # Enhancing positive weights by gamma\n",
        "    enhanced_weights = weights + gamma * weights_positive\n",
        "\n",
        "    # Computing contributions using activations and enhanced weights\n",
        "    z = np.dot(activations, enhanced_weights)  # Correct multiplication order\n",
        "\n",
        "    # Adding epsilon for numerical stability to avoid division by zero\n",
        "    epsilon = 1e-9\n",
        "    sum_z = np.sum(z) + epsilon * len(z)\n",
        "\n",
        "    # Computing the relevance scores for the lower layer\n",
        "    relevances_lower = np.zeros(activations.shape)\n",
        "    for i in range(len(activations)):\n",
        "        relevances_lower[i] = np.sum((activations[i] * enhanced_weights[i, :]) / sum_z * relevances_upper)\n",
        "\n",
        "    return relevances_lower\n",
        "\n",
        "activations = np.array([0.5, 0.2, 0.3])  # Example activations from the lower layer\n",
        "weights = np.array([[1, -1], [-1, 1], [0.5, 0.5]])  # Example weights to upper layer\n",
        "relevances_upper = np.array([0.6, 0.4])  # Example relevance scores from the upper layer\n",
        "\n",
        "# Apply the LRP-gamma rule\n",
        "relevances_lower = lrp_gamma_rule(activations, weights, relevances_upper, gamma=0.1)\n",
        "print(\"Relevances Lower Layer:\", relevances_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_hmlP1ETvOw",
        "outputId": "25880f89-0f3b-4ba0-8a09-6c5c785601ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevances Lower Layer: [ 0.325  -0.08    0.4125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhancing Positive Contributions:** The key step in the LRP-γ rule is the enhancement of positive weights by $( \\gamma )$. This is achieved by first isolating the positive parts of the weights $( w_{jk}^+ )$ and then adding $( \\gamma \\times w_{jk}^+ )$ back to the original weights. This emphasizes the contributions of positively activating connections.\n"
      ],
      "metadata": {
        "id": "WH-CpdtlT43p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing LRP"
      ],
      "metadata": {
        "id": "RtmHOpalDPBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficient implementation of LRP leverages the structure of neural networks and the modularity of propagation rules. This involves a forward pass to compute the activations, followed by backward passes where relevance scores are assigned to neurons based on the rules described above."
      ],
      "metadata": {
        "id": "149y69TaDSS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LRP in Different Layers"
      ],
      "metadata": {
        "id": "VAihE3TjDUWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LRP aims to offer insights into how input features influence the network's output by attributing relevance scores to these features. The choice of LRP rule (Basic LRP-0, LRP-ε for stabilization, or LRP-γ to emphasize positive contributions) at each layer can significantly affect the quality and interpretability of these explanations.\n",
        "\n",
        "- **Explanation Quality:** Optimal selection of LRP parameters (ε and γ) depends on achieving high explanation quality, focusing on fidelity (accuracy of representation) and understandability (ease of interpretation for humans).\n",
        "\n",
        "- **Uniform vs. Composite LRP:** Applying a single LRP rule uniformly across all layers can lead to explanations that are either too complex, too sparse, or misleading. A composite strategy, which employs different LRP rules at different layers, provides more balanced and insightful explanations.\n",
        "\n",
        "- **Layer-specific Strategy:**\n",
        "  - **Upper Layers:** Since these layers often deal with abstract concepts entangled across classes, using LRP-0 (close to the function gradient) can effectively untangle these contributions without oversimplification.\n",
        "  - **Middle Layers:** These layers benefit from the LRP-ε rule, which filters out noise and spurious variations, focusing on the most salient features.\n",
        "  - **Lower Layers:** LRP-γ is preferred here as it spreads relevance more uniformly across features, making explanations more intuitive and visually coherent to humans.\n",
        "\n",
        "- **Composite LRP Advantages:** This approach takes into account the distinct characteristics of different neural network layers, offering explanations that are both faithful to the model's decision process and easily interpretable by humans."
      ],
      "metadata": {
        "id": "dniftyZsDWmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extensions of LRP"
      ],
      "metadata": {
        "id": "I1-U7cklDZcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LRP is versatile and can extend beyond deep neural network classifiers to unsupervised models, time series prediction models like LSTMs, and models designed for pairwise matching tasks. Each of these applications requires nuanced modifications to the LRP framework to suit the specific type of model and task."
      ],
      "metadata": {
        "id": "rr2i5-MGBhVq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YY-5u0oYH46"
      },
      "source": [
        "# How LRP Works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GHSQ7JxYH46"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/lrp-how-it-runs.png\" width=\"800\" height=\"300\" alt=\"How LRP Works?\">\n",
        "    <figcaption>How LRP Works?</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sxtBmyBYH47"
      },
      "source": [
        "Layer-wise Relevance Propagation (LRP) is a technique used in machine learning to understand and visualize the contribution of each input feature to the final prediction of a neural network. It's particularly useful for deep learning models, where the decision-making process can be quite opaque.\n",
        "\n",
        "**Step 1: Forward Pass**\n",
        "\n",
        "- Start with a trained neural network ready for making predictions.\n",
        "- Input your data (like an image) into the network.\n",
        "- Perform a forward pass through the network, where the input is processed layer by layer to arrive at a final prediction. This step is just like any other prediction process in neural networks.\n",
        "\n",
        "**Step 2: Prediction Output**\n",
        "\n",
        "- Capture the output of the network. This could be a class label in classification tasks or a value in regression tasks.\n",
        "- Select the output neuron(s) corresponding to the predicted class or value for relevance backpropagation.\n",
        "\n",
        "**Step 3: Select Relevance**\n",
        "\n",
        "- The relevance is initially set to the output of the network for the predicted class. For instance, if the network predicts \"dog\", the relevance is assigned to the output neuron that corresponds to \"dog\".\n",
        "- All other output neurons are set to have zero relevance since they did not contribute to the final prediction.\n",
        "\n",
        "**Step 4: Backward Pass with LRP**\n",
        "\n",
        "- Perform a backward pass starting from the selected output neuron.\n",
        "- At each layer, distribute the relevance from the layer above to the neurons in the current layer. This is done using LRP rules which define how to allocate relevance among neurons based on their contributions.\n",
        "- Continue this process layer by layer, moving from the output layer towards the input layer.\n",
        "\n",
        "**Step 5: LRP Decomposition Rules**\n",
        "\n",
        "- Apply specific LRP rules at each layer to decompose the relevance among the connected neurons. These rules are based on the contribution of each neuron to the activation of the neurons in the next layer.\n",
        "- Commonly used rules include the LRP-ε, LRP-γ, and LRP-0 rules, each with different properties and applications.\n",
        "\n",
        "**Step 6: Relevance Conservation**\n",
        "\n",
        "- Ensure that the total relevance is conserved at each layer. The sum of the relevance scores assigned to all neurons in one layer should equal the total relevance from the layer above.\n",
        "\n",
        "**Step 7: Generation of Heatmaps**\n",
        "\n",
        "- Once the relevance has been backpropagated to the input layer, use the computed relevance scores to generate a heatmap.\n",
        "- This heatmap visualizes the contribution of each input feature (like pixels in an image) to the network’s prediction.\n",
        "\n",
        "**Step 8: Interpretation and Validation**\n",
        "\n",
        "- Analyze the heatmap to interpret which features were most and least important for the model's prediction.\n",
        "- Validate the explanation by techniques like pixel-perturbation analysis, where input features deemed important by LRP are altered to see if they affect the model's output.\n",
        "\n",
        "**Step 9: Evaluation of Explanations**\n",
        "\n",
        "- Use quantitative methods to evaluate the quality of the explanations provided by LRP, such as comparing them with ground truth annotations when available, or assessing the impact of perturbations on the model’s prediction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGnbRAd3-WAC"
      },
      "source": [
        "# Implementatation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "rDDixlMLa4f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will apply LRP to a brain cancer classification task using medical MRI scans.\n",
        "\n",
        "MRI stands for Magnetic Resonance Imaging, and LRP helps us not only to identify if there's brain cancer in an image but also to understand why the model made its prediction. LRP is predominantly applied to neural networks but can be used with support vector machines as well.\n",
        "\n",
        "Our goal with LRP is to visually explain the decision of a model by showing which parts of an input, such as image pixels, contributed to a particular prediction.\n",
        "\n",
        "For the implementation I'll be using the [xai-series](https://github.com/deepfindr/xai-series/tree/master) implementation in https://github.com/deepfindr/xai-series/blob/master/05_lrp.py"
      ],
      "metadata": {
        "id": "hQVyvSewYcCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "j9QGZamhYvRy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding, it's important to check if a GPU is available for computation, as this will significantly speed up the training process."
      ],
      "metadata": {
        "id": "hqUJPiAKY9gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set GPU device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7GUORfdZELF",
        "outputId": "6dd165ea-22f0-401f-9b36-1aed287e7a61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Downloading"
      ],
      "metadata": {
        "id": "wSZc3Oyda8a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will be using a dataset of brain MRI images. These images are organized into separate folders based on whether they show evidence of a tumor. The dataset contains both a training and a testing set, which I will load for our model."
      ],
      "metadata": {
        "id": "hotFo9O9ZUSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle -q"
      ],
      "metadata": {
        "id": "H-qdTCapbBgM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt the user for API username and key input\n",
        "kaggle_username = getpass('Enter your Kaggle username')\n",
        "kaggle_key = getpass('Enter your Kaggle API key')\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_username  # Sets the username as an environment variable\n",
        "os.environ['KAGGLE_KEY'] = kaggle_key            # Sets the key as an environment variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snqMvtHUcAaB",
        "outputId": "9b177984-b30a-4f8f-8e7b-899d43cd672b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Kaggle username··········\n",
            "Enter your Kaggle API key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d sartajbhuvaji/brain-tumor-classification-mri"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdwe8bpAbKQJ",
        "outputId": "82ae0370-f3a2-4555-932c-7777b7ec71dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading brain-tumor-classification-mri.zip to /content\n",
            " 98% 85.0M/86.8M [00:00<00:00, 181MB/s]\n",
            "100% 86.8M/86.8M [00:00<00:00, 149MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_ROOT = \"./data/brain_mri/Training\"\n",
        "TEST_ROOT = \"./data/brain_mri/Testing\""
      ],
      "metadata": {
        "id": "LIGr2G45cwF4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"./data/brain_mri\"  # Creates the directory and any necessary parent directories\n",
        "!unzip -q brain-tumor-classification-mri.zip -d \"./data/brain_mri\""
      ],
      "metadata": {
        "id": "qxVo3Fkwctin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Load data\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_ROOT)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root=TRAIN_ROOT)"
      ],
      "metadata": {
        "id": "iDC7f75UZeGS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model"
      ],
      "metadata": {
        "id": "6pr6QEPAd6dB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct our neural network model based on the pre-trained VGG16 architecture, modifying the output layer to suit our four-class problem - identifying different types of brain tumors. This model adaptation allows us to leverage the powerful feature extraction capabilities of VGG16 while fine-tuning it for our specific task."
      ],
      "metadata": {
        "id": "V1Ts1001eBWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Building the model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Replace output layer according to our problem\n",
        "        in_feats = self.vgg16.classifier[6].in_features\n",
        "        self.vgg16.classifier[6] = nn.Linear(in_feats, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg16(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_YEl2HWeFGy",
        "outputId": "84f19de2-2ae6-4937-ec55-cbc1cc083f22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 102MB/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (vgg16): VGG(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): ReLU(inplace=True)\n",
              "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (18): ReLU(inplace=True)\n",
              "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (20): ReLU(inplace=True)\n",
              "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (22): ReLU(inplace=True)\n",
              "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (25): ReLU(inplace=True)\n",
              "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (27): ReLU(inplace=True)\n",
              "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (29): ReLU(inplace=True)\n",
              "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "    (classifier): Sequential(\n",
              "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): Linear(in_features=4096, out_features=4, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "S3NlH2BPeMT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we apply the necessary transformations to our dataset images to fit the input requirements of the pre-trained VGG16 model. The images are resized and converted to tensors, ready for model consumption."
      ],
      "metadata": {
        "id": "CZ27BsUSeWaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Prepare data for pretrained model\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=TRAIN_ROOT,\n",
        "        transform=transforms.Compose([\n",
        "                      transforms.Resize((255,255)),\n",
        "                      transforms.ToTensor()\n",
        "        ])\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=TEST_ROOT,\n",
        "        transform=transforms.Compose([\n",
        "                      transforms.Resize((255,255)),\n",
        "                      transforms.ToTensor()\n",
        "        ])\n",
        ")"
      ],
      "metadata": {
        "id": "i6Deu13YeZaM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Data Loaders"
      ],
      "metadata": {
        "id": "JANJO3JDedpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loaders are an integral part of the training process, enabling efficient data manipulation and batching. Here, we define our data loaders for both the training and test sets with a batch size of 32."
      ],
      "metadata": {
        "id": "353mUlbeeha1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "NuxY0dG9egqI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "709OWfZFe0gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our neural network model using the cross-entropy loss function and the Adam optimizer. The learning rate is set to a small value to fine-tune the pre-trained network. We will train the model for 10 epochs and print the loss for each epoch to monitor the training progress."
      ],
      "metadata": {
        "id": "nyPne9kke3dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Train\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "epochs = 10\n",
        "\n",
        "# Iterate x epochs over the train data\n",
        "for epoch in range(epochs):\n",
        "    for i, batch in enumerate(train_loader, 0):\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        # Labels are automatically one-hot-encoded\n",
        "        loss = cross_entropy_loss(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsPu0y-Ge7r9",
        "outputId": "d27f3e63-beda-42f5-dadc-260d29aac4cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.4567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.5099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.4097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.4625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting Predictions"
      ],
      "metadata": {
        "id": "7xbGaTN6fFeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, it's crucial to evaluate the model's performance. Here, we load a batch of test data and make predictions with the trained model. We calculate the batch accuracy by comparing the predicted labels with the ground truth and display the results in a dataframe for a clear comparison."
      ],
      "metadata": {
        "id": "ZkJfW9lBfJWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Inspect predictions for first batch\n",
        "inputs, labels = next(iter(test_loader))\n",
        "inputs = inputs.to(device)\n",
        "labels = labels.numpy()\n",
        "outputs = model(inputs).max(1).indices.detach().cpu().numpy()\n",
        "comparison = pd.DataFrame()\n",
        "print(\"Batch accuracy: \", (labels==outputs).sum()/len(labels))\n",
        "comparison[\"labels\"] = labels\n",
        "comparison[\"outputs\"] = outputs\n",
        "comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WdZudvljfNEF",
        "outputId": "1f160048-36fe-45ab-d180-6e0b0e86639b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch accuracy:  0.8125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    labels  outputs\n",
              "0        2        2\n",
              "1        2        2\n",
              "2        1        1\n",
              "3        2        2\n",
              "4        1        1\n",
              "5        0        1\n",
              "6        2        2\n",
              "7        0        0\n",
              "8        2        2\n",
              "9        2        1\n",
              "10       3        3\n",
              "11       0        1\n",
              "12       1        1\n",
              "13       2        2\n",
              "14       1        1\n",
              "15       3        3\n",
              "16       1        1\n",
              "17       0        1\n",
              "18       1        1\n",
              "19       3        3\n",
              "20       1        1\n",
              "21       2        2\n",
              "22       1        1\n",
              "23       0        0\n",
              "24       2        2\n",
              "25       2        2\n",
              "26       3        2\n",
              "27       1        1\n",
              "28       0        1\n",
              "29       3        3\n",
              "30       1        1\n",
              "31       3        3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88829ade-06b9-4bfb-95c5-fd15d2b41701\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>outputs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88829ade-06b9-4bfb-95c5-fd15d2b41701')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88829ade-06b9-4bfb-95c5-fd15d2b41701 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88829ade-06b9-4bfb-95c5-fd15d2b41701');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6c1a2ade-1b98-4523-80f8-780ce7db9876\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c1a2ade-1b98-4523-80f8-780ce7db9876')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6c1a2ade-1b98-4523-80f8-780ce7db9876 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison",
              "summary": "{\n  \"name\": \"comparison\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"outputs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Layerwise Relevance Propagation (LRP)"
      ],
      "metadata": {
        "id": "-Oo9zxx4fRUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layerwise Relevance Propagation (LRP) aims to explain the decision-making process of neural networks. We start by defining functions to adapt our VGG16 model for LRP by transforming dense layers to convolutional ones and cloning layers while applying specific relevance rules. We then compute the relevances for each layer in reverse, starting from the output and working back to the input, essentially performing a backward pass of relevance scores.\n",
        "\n",
        "LRP assigns a relevance score to each neuron based on its contribution to the final decision. This score is backpropagated through the network's layers to the input layer. We treat different layers with specific rules - for lower layers, we focus on positive contributions, while for upper layers, we apply a more general rule without bias.\n",
        "\n",
        "This process helps us understand which parts of an input image the model finds most relevant for its prediction, giving us insight into its decision-making process."
      ],
      "metadata": {
        "id": "rDy6KdY8fU1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting dense (fully connected) layers to convolutional layers within the context of Layer-wise Relevance Propagation (LRP) for models like VGG16 is a technique primarily used for the reasons below:\n",
        "\n",
        "- **Preservation of Spatial Context:** Convolutional layers inherently maintain spatial relationships within their input data, which is crucial for image data where the location of features within the image affects the interpretation. When applying LRP to convolutional neural networks (CNNs), preserving this spatial context as relevance scores are propagated back through the network is vital for generating meaningful explanations. By converting dense layers to convolutional layers, the spatial context of the activations can be maintained and utilized in the relevance propagation process.\n",
        "\n",
        "- **Adapting to Different Input Sizes:** Another practical reason for this conversion is to allow the network to adapt to different input sizes. Convolutional operations are not dependent on the input size, whereas dense layers require a fixed-size input. This conversion can make the LRP process more flexible and applicable to various input dimensions without losing the interpretability of relevance scores across different layers of the network.\n",
        "\n",
        "- **Simplification of the LRP Algorithm:** Applying LRP through a network that consists solely of convolutional operations (including those converted from dense layers) simplifies the process. It allows the use of a unified approach for propagating relevance scores from the output back to the input. This simplification can make the implementation of LRP more straightforward and its explanations more consistent across different types of layers.\n",
        "\n",
        "\n",
        "**Conversion Process:** The conversion process involves reshaping the weights of the dense layer to match the dimensions expected by a convolutional operation. For example, the first dense layer in VGG16, which follows a series of convolutional and pooling layers, receives an input that can be viewed as a single-dimensional vector per feature map but is conceptually a flattened version of the spatial feature maps produced by the preceding convolutional layers. By converting this dense layer to a convolutional layer, we essentially \"unflatten\" this vector back into a format that preserves the original spatial dimensions of the feature maps.\n",
        "\n",
        "This technique ensures that the explanatory process of LRP remains consistent and interpretable across the entire architecture, especially in CNNs designed for tasks like image classification where spatial information is crucial."
      ],
      "metadata": {
        "id": "KaAltpiaXWeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_to_conv(layers):\n",
        "    \"\"\" Converts a dense layer to a conv layer \"\"\"\n",
        "    newlayers = []\n",
        "    for i,layer in enumerate(layers):\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            newlayer = None\n",
        "            if i == 0:\n",
        "                m, n = 512, layer.weight.shape[0]\n",
        "                newlayer = nn.Conv2d(m,n,7)\n",
        "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,7,7))\n",
        "            else:\n",
        "                m,n = layer.weight.shape[1],layer.weight.shape[0]\n",
        "                newlayer = nn.Conv2d(m,n,1)\n",
        "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,1,1))\n",
        "            newlayer.bias = nn.Parameter(layer.bias)\n",
        "            newlayers += [newlayer]\n",
        "        else:\n",
        "            newlayers += [layer]\n",
        "    return newlayers"
      ],
      "metadata": {
        "id": "C1X-Gwi1Xppp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `new_layer(layer, g)` is used in the context of Layer-wise Relevance Propagation (LRP) for a few significant reasons. It's a crucial part of preparing the neural network layers for the propagation of relevance scores from the output back to the input, ensuring the explanations reflect the contributions of input features to the final decision made by the model. Here's a breakdown of its purposes and benefits:\n",
        "\n",
        "1. **Layer Modification for LRP:**\n",
        "- The function allows for the modification of layer parameters according to a specific transformation function `g`. This is essential for LRP, where different rules or transformations might be applied to different layers to compute the relevance scores accurately.\n",
        "\n",
        "2. **Preserving Original Network Architecture:**\n",
        "- By cloning the layer with `copy.deepcopy(layer)`, the function ensures that the original network architecture is preserved. This is crucial because LRP involves manipulating layer weights and biases to compute relevance scores, and these manipulations should not alter the original model parameters that were learned during training.\n",
        "\n",
        "3. **Flexible Application of Transformations:**\n",
        "- The use of a transformation function `g` that is applied to the layer's weights and biases provides a flexible framework for implementing various LRP rules. Since different layers or parts of the network might require different treatment under LRP (e.g., handling of positive vs. negative weights, application of ε-rule or γ-rule), this function enables a systematic and customizable approach to apply these rules across the network."
      ],
      "metadata": {
        "id": "Nw2jxnqYYQIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_layer(layer, g):\n",
        "    \"\"\"Clone a layer and pass its parameters through the function g.\"\"\"\n",
        "    layer = copy.deepcopy(layer)\n",
        "    try:\n",
        "      layer.weight = torch.nn.Parameter(g(layer.weight))\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      layer.bias = torch.nn.Parameter(g(layer.bias))\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return layer"
      ],
      "metadata": {
        "id": "WGyfRf4PYvdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get linear layer indices"
      ],
      "metadata": {
        "id": "D7h1Fli-bT-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_linear_layer_indices(model):\n",
        "  '''\n",
        "  The function get_linear_layer_indices(model) is designed to identify and\n",
        "  return the indices of linear (fully connected) layers within a specified\n",
        "  model, specifically within the context of a VGG16 architecture.\n",
        "  '''\n",
        "    # The VGG16 architecture is divided into two main parts: features\n",
        "    # (convolutional layers) and classifier (linear layers). The offset\n",
        "    # is used to adjust the indices of the linear layers to reflect their\n",
        "    # position in the entire model (including both the features and classifier parts).\n",
        "    offset = len(model.vgg16._modules['features']) + 1\n",
        "\n",
        "    indices = []\n",
        "    for i, layer in enumerate(model.vgg16._modules['classifier']):\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            indices.append(i)\n",
        "\n",
        "    # This adjustment is necessary to reflect the linear layers' actual\n",
        "    # positions within the entire VGG16 model structure\n",
        "    indices = [offset + val for val in indices]\n",
        "\n",
        "    return indices"
      ],
      "metadata": {
        "id": "CGw86Xy_ZF78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Layer-wise Relevance Propagation (LRP) on a VGG16 model for a given image."
      ],
      "metadata": {
        "id": "rYsj8YrobSHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "def apply_lrp_on_vgg16(model, image):\n",
        "    \"\"\"\n",
        "    Apply Layer-wise Relevance Propagation (LRP) on a VGG16 model for a given image.\n",
        "\n",
        "    This function propagates an input image through the VGG16 model to compute relevance scores,\n",
        "    explaining the contribution of each input pixel to the final decision. It modifies the model's\n",
        "    dense layers to convolutional layers for uniform processing and applies different LRP rules\n",
        "    based on the layer depth to enhance the explanation's fidelity and understandability.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): Pre-trained VGG16 model.\n",
        "    - image (torch.Tensor): Input image tensor.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Relevance scores for the input image, indicating the contribution of each pixel.\n",
        "    \"\"\"\n",
        "\n",
        "    # Add a batch dimension to the image if not already present\n",
        "    image = torch.unsqueeze(image, 0)\n",
        "\n",
        "    # Extract and modify the model's layers for LRP\n",
        "    # Concatenates convolutional layers from 'features', adaptive pooling, and modified dense layers\n",
        "    layers = list(model.vgg16._modules['features']) + \\\n",
        "             [model.vgg16._modules['avgpool']] + \\\n",
        "             dense_to_conv(list(model.vgg16._modules['classifier']))\n",
        "    linear_layer_indices = get_linear_layer_indices(model)\n",
        "\n",
        "    # Initialize a list to store activations at each layer\n",
        "    activations = [image] + [None] * len(layers)\n",
        "\n",
        "    # Propagate the image through the layers and store the activations\n",
        "    for i, layer in enumerate(layers):\n",
        "        # Special handling for layers that were originally dense\n",
        "        if i in linear_layer_indices and i == 32:\n",
        "            # Reshape for compatibility with convolutional operation\n",
        "            activations[i] = activations[i].reshape((1, 512, 7, 7))\n",
        "        # Forward pass through the current layer\n",
        "        activation = layer.forward(activations[i])\n",
        "        # Flatten if the layer is adaptive average pooling\n",
        "        if isinstance(layer, torch.nn.modules.pooling.AdaptiveAvgPool2d):\n",
        "            activation = torch.flatten(activation, start_dim=1)\n",
        "        # Store the activation for the next layer\n",
        "        activations[i + 1] = activation\n",
        "\n",
        "    # Prepare the output of the model for LRP by converting it to one-hot encoding\n",
        "    output_activation = activations[-1].detach().cpu().numpy()\n",
        "    max_activation = output_activation.max()\n",
        "    one_hot_output = [val if val == max_activation else 0 for val in output_activation[0]]\n",
        "    # Replace the last layer activation with the one-hot encoded vector\n",
        "    # (Uncomment and modify the following line to match your device, e.g., \".to(device)\")\n",
        "    # activations[-1] = torch.FloatTensor([one_hot_output]).to(device)\n",
        "\n",
        "    # Backpropagate relevance scores through the network\n",
        "    relevances = [None] * len(layers) + [activations[-1]]\n",
        "    for i in reversed(range(len(layers))):\n",
        "        layer = layers[i]\n",
        "        # For max pooling layers, treat them as average pooling for relevance propagation\n",
        "        if isinstance(layer, torch.nn.MaxPool2d):\n",
        "            layer = torch.nn.AvgPool2d(2)\n",
        "\n",
        "        if isinstance(layer, (torch.nn.Conv2d, torch.nn.AvgPool2d, torch.nn.Linear)):\n",
        "            # Enable gradient calculation for relevance propagation\n",
        "            activations[i] = activations[i].data.requires_grad_(True)\n",
        "\n",
        "            # Apply specific LRP rules based on layer depth\n",
        "            if i <= 16:\n",
        "                rho = lambda p: p + 0.25 * p.clamp(min=0)\n",
        "                incr = lambda z: z + 1e-9\n",
        "            elif 17 <= i <= 30:\n",
        "                rho = lambda p: p\n",
        "                incr = lambda z: z + 1e-9 + 0.25 * ((z**2).mean()**.5).data\n",
        "            else:  # Upper layers\n",
        "                rho = lambda p: p\n",
        "                incr = lambda z: z + 1e-9\n",
        "\n",
        "            # Transform layer weights according to the LRP rule and propagate relevance\n",
        "            z = incr(new_layer(layer, rho).forward(activations[i]))\n",
        "            s = (relevances[i + 1] / z).data\n",
        "            (z * s).sum().backward()\n",
        "            c = activations[i].grad\n",
        "            # Update relevance scores\n",
        "            relevances[i] = (activations[i] * c).data\n",
        "        else:\n",
        "            relevances[i] = relevances[i + 1]\n",
        "\n",
        "    # Return the relevance scores corresponding to the input image\n",
        "    return relevances[0]"
      ],
      "metadata": {
        "id": "sl3j0fPDfYI9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Relevance Scores"
      ],
      "metadata": {
        "id": "CjdjbNAgfgZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the LRP implementation, we can now visualize the relevance scores for individual images. This cell demonstrates how to apply LRP to a specific test image and generate a heatmap of relevances, which helps us see which pixels were most influential in the model's prediction.\n",
        "\n",
        "We normalize the relevance scores to make them easier to visualize and use a color map to distinguish areas of high and low relevance. The resulting visualization can be compared with the original image to better understand the model's decision-making process. This kind of visualization is particularly useful in domains like medical imaging, where understanding the model's focus can be critical."
      ],
      "metadata": {
        "id": "gyoVPpL_fjW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Calculate relevances for first image in this test batch\n",
        "image_id = 2\n",
        "image_relevances = apply_lrp_on_vgg16(model, inputs[image_id])\n",
        "image_relevances = image_relevances.permute(0,2,3,1).detach().cpu().numpy()[0]\n",
        "# Normalize the relevance scores to make them easier to visualize\n",
        "image_relevances = np.interp(image_relevances, (image_relevances.min(),\n",
        "                                                image_relevances.max()),\n",
        "                                                (0, 1))\n",
        "# Show relevances\n",
        "pred_label = list(test_dataset.class_to_idx.keys())[\n",
        "             list(test_dataset.class_to_idx.values())\n",
        "            .index(labels[image_id])]\n",
        "if outputs[image_id] == labels[image_id]:\n",
        "    print(\"Groundtruth for this image: \", pred_label)\n",
        "\n",
        "    # Plot images next to each other\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1,2,1)\n",
        "    # Use a color map to distinguish areas of high and low relevance\n",
        "    plt.imshow(image_relevances[:,:,0], cmap=\"seismic\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(inputs[image_id].permute(1,2,0).detach().cpu().numpy())\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"This image is not classified correctly.\")"
      ],
      "metadata": {
        "id": "NfB1hv1BfmmT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUPmnD3g9w_l"
      },
      "source": [
        "# Explainable AI Demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnp4iIYb9w_l"
      },
      "source": [
        "[Explainable AI Demos](https://lrpserver.hhi.fraunhofer.de/) is an educational or demonstration tool designed to make AI more accessible and understandable to users by visually and interactively showcasing how AI models arrive at their conclusions. There are four different types of demos:\n",
        "\n",
        "- Handwriting Classification: This demo seems to use LRP to explain how a neural network trained on the MNIST dataset predicts handwritten digits. It suggests that users can input their handwriting for the AI to classify and explain.\n",
        "- Image Classification: A more advanced LRP demo for image classification that uses a neural network implemented with Caffe, a deep learning framework. This demo likely illustrates how the AI model determines the content of images.\n",
        "- Text Classification: This is for classifying natural language documents. The neural network provides predictions on the document's semantic category and uses LRP to explain the classification process.\n",
        "- Visual Question Answering: This demo allows users to ask AI questions about an image and receive not only answers but also visual explanations that highlight relevant parts of the image involved in the AI's reasoning.\n",
        "<table>\n",
        "<tr>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/lrp_text_recognition_and_classification.png\" alt=\"Text Recognition and Classification\" width=\"600\" /></td>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/lrp_image_classification.png\" alt=\"Image Classification\" width=\"600\" /></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/lrp_mnist_image_recognition.png\" alt=\"MNIST Image Recognition\" width=\"600\" /></td>\n",
        "    <td style=\"padding: 10px;\"><img src=\"https://raw.githubusercontent.com/arkeodev/XAI/main/images/lrp_question_and_answering.png\" alt=\"Question and Answering\" width=\"600\" /></td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEuibzQc-WAC"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlC4T3Gn-WAC"
      },
      "source": [
        "- Layer-Wise Relevance Propagation: An Overview (Paper): https://iphome.hhi.de/samek/pdf/MonXAI19.pdf\n",
        "- Layer-Wise Relevance Propogation: https://www.hhi.fraunhofer.de/en/departments/ai/technologies-and-solutions/layer-wise-relevance-propagation.html\n",
        "- Explainable AI Demos: https://lrpserver.hhi.fraunhofer.de/\n",
        "- deepfindr LRP Implementation: https://github.com/deepfindr/xai-series/blob/master/05_lrp.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}